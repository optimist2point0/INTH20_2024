{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8331746a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import json\n",
    "\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "import mediapipe as mp\n",
    "from mtcnn import MTCNN\n",
    "from deepface import DeepFace\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "import keras.backend as K\n",
    "\n",
    "from keras.optimizers.legacy import Adam\n",
    "from keras.losses import binary_crossentropy\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "from skimage.io import imread\n",
    "from skimage.transform import resize\n",
    "from skimage.color import gray2rgb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5671e172",
   "metadata": {},
   "source": [
    "# Part 1 - Check what image is anfas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "622b0714",
   "metadata": {},
   "outputs": [],
   "source": [
    "# folder of init images\n",
    "TRAIN_FOLDER = \"wiki_crop\"\n",
    "# key:image_path, value:box of face and other\n",
    "IMAGES_INFO = {}\n",
    "# folder for new crop image\n",
    "CROPPED_FOLDER = \"cropped/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdde6a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize MediaPipe Face Mesh.\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "face_mesh = mp_face_mesh.FaceMesh(min_detection_confidence=0.5, min_tracking_confidence=0.5)\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "drawing_spec = mp_drawing.DrawingSpec(color=(128, 0, 128), thickness=2, circle_radius=1)\n",
    "\n",
    "\n",
    "def get_head_pos(image_path, tresh):\n",
    "    # Load an image.\n",
    "    image = cv2.imread(image_path)  # Make sure to use the correct path to your image.\n",
    "\n",
    "    # Convert the color from BGR to RGB.\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    image.flags.writeable = False\n",
    "\n",
    "    # Process the image.\n",
    "    results = face_mesh.process(image)\n",
    "\n",
    "    image.flags.writeable = True\n",
    "\n",
    "    # Convert the color from RGB to BGR.\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "    img_h, img_w, img_c = image.shape\n",
    "    face_2d = []\n",
    "    face_3d = []\n",
    "\n",
    "    if results.multi_face_landmarks:\n",
    "        for face_landmarks in results.multi_face_landmarks:\n",
    "            for idx, lm in enumerate(face_landmarks.landmark):\n",
    "                if idx == 33 or idx == 263 or idx == 1 or idx == 61 or idx == 291 or idx == 199:\n",
    "                    if idx == 1:\n",
    "                        nose_2d = (lm.x * img_w, lm.y * img_h)\n",
    "                        nose_3d = (lm.x * img_w, lm.y * img_h, lm.z * 3000)\n",
    "                    x, y = int(lm.x * img_w), int(lm.y * img_h)\n",
    "\n",
    "                    face_2d.append([x, y])\n",
    "                    face_3d.append(([x, y, lm.z]))\n",
    "\n",
    "            # Convert to numpy array for CV2 processing.\n",
    "            face_2d = np.array(face_2d, dtype=np.float64)\n",
    "            face_3d = np.array(face_3d, dtype=np.float64)\n",
    "\n",
    "            focal_length = 1 * img_w\n",
    "            cam_matrix = np.array([[focal_length, 0, img_h / 2],\n",
    "                                  [0, focal_length, img_w / 2],\n",
    "                                  [0, 0, 1]])\n",
    "            distortion_matrix = np.zeros((4, 1), dtype=np.float64)\n",
    "\n",
    "            # Solve the PnP problem.\n",
    "            success, rotation_vec, translation_vec = cv2.solvePnP(face_3d, face_2d, cam_matrix, distortion_matrix)\n",
    "\n",
    "            rmat, jac = cv2.Rodrigues(rotation_vec)\n",
    "            angles, mtxR, mtxQ, Qx, Qy, Qz = cv2.RQDecomp3x3(rmat)\n",
    "\n",
    "            x = angles[0] * 360\n",
    "            y = angles[1] * 360\n",
    "            z = angles[2] * 360\n",
    "\n",
    "            # Determine the text based on the angles.\n",
    "            text = \"Forward\"\n",
    "            if y < -tresh:\n",
    "                text = \"Looking Left\"\n",
    "            elif y > tresh:\n",
    "                text = \"Looking Right\"\n",
    "            elif x < -tresh:\n",
    "                text = \"Looking Down\"\n",
    "            elif x > tresh:\n",
    "                text = \"Looking Up\"\n",
    "            \n",
    "            return text\n",
    "\n",
    "    else:\n",
    "        return \"idk\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f1748ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# result list of image pathes that \"look forward\"\n",
    "result_list = []\n",
    "for folder in os.listdir(TRAIN_FOLDER):\n",
    "    # if it is directory\n",
    "    if os.path.isdir(os.path.join(TRAIN_FOLDER, folder)):\n",
    "        \n",
    "        # path to images in that directory\n",
    "        images_path = os.path.join(TRAIN_FOLDER, folder)\n",
    "        \n",
    "        for img_name in os.listdir(images_path):\n",
    "            \n",
    "            # image path\n",
    "            image_path = os.path.join(images_path, img_name)\n",
    "            res = get_head_pos(image_path, 15) \n",
    "            # print(res)\n",
    "            if res == \"Forward\":\n",
    "                result_list.append(image_path)\n",
    "            if len(result_list) % 100 == 0:\n",
    "                print(len(result_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c82e81a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # alternative save as txt\n",
    "\n",
    "# # File path\n",
    "# file_path = 'full_face_list.txt'\n",
    "\n",
    "# # Write the list to a file\n",
    "# with open(file_path, 'w') as file:\n",
    "#     for item in result_list:\n",
    "#         file.write(f\"{item}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "016ea4e0",
   "metadata": {},
   "source": [
    "# Part 2 - Crop image with MTCNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f64443d",
   "metadata": {},
   "source": [
    "## Detect box of faces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "968830b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # opening the file in read mode \n",
    "# my_file = open(file_path, \"r\") \n",
    "  \n",
    "# # reading the file \n",
    "# data = my_file.read() \n",
    "  \n",
    "# # replacing end splitting the text  \n",
    "# # when newline ('\\n') is seen. \n",
    "# data_into_list = data.split(\"\\n\")\n",
    "# my_file.close() \n",
    "\n",
    "# # if you also have \".DS_Store\" file\n",
    "# data_into_list.pop(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "189b45a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for time consumption\n",
    "start_time = time.time()\n",
    "\n",
    "# model for face detection\n",
    "detector = MTCNN()\n",
    "\n",
    "\n",
    "for image_path in result_list:\n",
    "\n",
    "                \n",
    "    # open image\n",
    "    img = cv2.cvtColor(cv2.imread(image_path), cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # save model predict\n",
    "    IMAGES_INFO[image_path] = detector.detect_faces(img)\n",
    "\n",
    "            \n",
    "\n",
    "print(\"Time of execution:\", time.time()-start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fea59dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # alternative save as json\n",
    "# # Specify the filename to save the dictionary as a JSON file\n",
    "# filename = 'IMAGES_INFO_FINAL.json'\n",
    "\n",
    "# # Writing JSON data\n",
    "# with open(filename, 'w') as f:\n",
    "#     json.dump(IMAGES_INFO, f)\n",
    "\n",
    "# print(f\"Dictionary saved as JSON in {filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5149ec65",
   "metadata": {},
   "source": [
    "## Cropping and saving new images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a43b9509",
   "metadata": {},
   "outputs": [],
   "source": [
    "# file = open(filename)\n",
    "\n",
    "# IMAGES_INFO = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c47f8e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "size_mean = ([], [])\n",
    "\n",
    "for image_path in IMAGES_INFO.keys():\n",
    "    try:\n",
    "        x, y, h, w = IMAGES_INFO[image_path][0]['box']\n",
    "    \n",
    "        # Read the image into memory\n",
    "        image = cv2.imread(image_path)\n",
    "        \n",
    "        cropped_image = image[y:(y+w), x:(x+h)]\n",
    "        \n",
    "        size_mean[0].append(cropped_image.shape[0])\n",
    "        size_mean[1].append(cropped_image.shape[1])\n",
    "        \n",
    "        cv2.imwrite(os.path.join(CROPPED_FOLDER,image_path[13:]), cropped_image)\n",
    "    except:\n",
    "        print('pass')\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9413703",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mean size of image\n",
    "(np.mean(size_mean[0]), np.mean(size_mean[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2270a8e5",
   "metadata": {},
   "source": [
    "# Part 3 - Classification images with Deepface for segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b5b580",
   "metadata": {},
   "outputs": [],
   "source": [
    "#key:image_path, value:results of classification\n",
    "result_dict = {}\n",
    "\n",
    "all_images = os.listdir(CROPPED_FOLDER)\n",
    "\n",
    "for i, path in enumerate(all_images):\n",
    "    if path[0] == '.':\n",
    "        # print(path)\n",
    "        # print(i)\n",
    "        all_images.pop(i)\n",
    "\n",
    "for ind, image_name in enumerate(all_images):\n",
    "    image_path = CROPPED_FOLDER + \"/\" + image_name\n",
    "    # print(image_path)\n",
    "    objs = DeepFace.analyze(\n",
    "        img_path= image_path,\n",
    "        actions= ['age', 'gender', 'race', 'emotion'],\n",
    "        enforce_detection= False\n",
    "    )\n",
    "    result_dict[image_name] = {\n",
    "        'age': objs[0]['age'],\n",
    "        'gender': objs[0]['dominant_gender'], \n",
    "        'race': objs[0]['dominant_race'],\n",
    "        'emotion': objs[0]['dominant_emotion'],\n",
    "    }\n",
    "    if ind % 100 == 0:\n",
    "        print(ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29f5efa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # alternative save as json\n",
    "# # Specify the filename to save the dictionary as a JSON file\n",
    "# filename = 'crop_photo_classes.json'\n",
    "\n",
    "# # Writing JSON data\n",
    "# with open(filename, 'w') as f:\n",
    "#     json.dump(result_dict, f)\n",
    "\n",
    "# print(f\"Dictionary saved as JSON in {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c614b1d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lists of image_path for different classes\n",
    "\n",
    "men = []\n",
    "women = []\n",
    "\n",
    "\n",
    "age_25 = []\n",
    "age_25_30 = []\n",
    "age_30_40 = []\n",
    "age_50 = []\n",
    "\n",
    "asian = []\n",
    "white = []\n",
    "middle_eastern = []\n",
    "indian = []\n",
    "latino = []\n",
    "black = []\n",
    "\n",
    "\n",
    "angry = []\n",
    "fear = []\n",
    "neutral = []\n",
    "sad = []\n",
    "disgust = []\n",
    "happy = []\n",
    "surprise = []\n",
    "\n",
    "\n",
    "for key in data.keys():\n",
    "    if data[key][\"gender\"] == \"Man\":\n",
    "        men.append(key)\n",
    "    else:\n",
    "        women.append(key)\n",
    "\n",
    "\n",
    "    if data[key][\"race\"] == \"asian\":\n",
    "        asian.append(key)\n",
    "    elif data[key][\"race\"] == \"white\":\n",
    "        white.append(key)\n",
    "    elif data[key][\"race\"] == \"middle eastern\":\n",
    "        middle_eastern.append(key)\n",
    "    elif data[key][\"race\"] == \"indian\":\n",
    "        indian.append(key)\n",
    "    elif data[key][\"race\"] == \"latino hispanic\":\n",
    "        latino.append(key)\n",
    "    else:\n",
    "        black.append(key)\n",
    "\n",
    "    if data[key][\"emotion\"] == \"happy\":\n",
    "        happy.append(key)\n",
    "    elif data[key][\"emotion\"] == \"neutral\":\n",
    "        neutral.append(key)\n",
    "    elif data[key][\"emotion\"] == \"fear\":\n",
    "        fear.append(key)\n",
    "    elif data[key][\"emotion\"] == \"sad\":\n",
    "        sad.append(key)\n",
    "    elif data[key][\"emotion\"] == \"angry\":\n",
    "        angry.append(key)\n",
    "    elif data[key][\"emotion\"] == \"surprise\":\n",
    "        surprise.append(key)\n",
    "    else:\n",
    "        disgust.append(key)\n",
    "\n",
    "    if data[key][\"age\"] <= 25:\n",
    "        age_25.append(key)\n",
    "    elif data[key][\"age\"] <= 30:\n",
    "        age_25_30.append(key)\n",
    "    elif data[key][\"age\"] <= 40:\n",
    "        age_30_40.append(key)\n",
    "    else:\n",
    "        age_50.append(key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15c2719e",
   "metadata": {},
   "source": [
    "# Part 4 - Fit Autoencoders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78824efb",
   "metadata": {},
   "source": [
    "## Conv Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "567d3942",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_WEIGHTS_PATH = 'models_weigths/'\n",
    "\n",
    "\n",
    "#Training parameters\n",
    "VALID_SIZE = 0.1\n",
    "N_EPOCHS = 15\n",
    "BATCH_SIZE = 32\n",
    "IMAGE_RESIZE = (256, 256)\n",
    "\n",
    "#random seed\n",
    "RANDOM_SEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b38f4e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_images = os.listdir(CROPPED_FOLDER)\n",
    "\n",
    "for i, path in enumerate(all_images):\n",
    "    if path[0] == '.':\n",
    "        print(path)\n",
    "        print(i)\n",
    "        all_images.pop(i)\n",
    "\n",
    "#split images \n",
    "train_filenames, valid_filenames = train_test_split(\n",
    "    all_images, \n",
    "    test_size=VALID_SIZE,\n",
    "    random_state=RANDOM_SEED\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f22a542a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Train sample size: ', len(train_filenames))\n",
    "print('Valid sample size: ', len(valid_filenames))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac4f9707",
   "metadata": {},
   "source": [
    "### Data generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1645df55",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data generator for images\n",
    "\n",
    "class generator(keras.utils.Sequence):\n",
    "    \n",
    "    def __init__(self, folder, filenames, batch_size=BATCH_SIZE, image_size=IMAGE_RESIZE, shuffle=True, predict=False):\n",
    "        self.folder = folder\n",
    "        self.filenames = filenames\n",
    "        self.batch_size = batch_size\n",
    "        self.image_size = image_size\n",
    "        self.shuffle = shuffle\n",
    "        self.predict = predict\n",
    "        self.on_epoch_end()\n",
    "        \n",
    "    def __load__(self, filename):\n",
    "        # load jpg file as numpy array\n",
    "        img = imread(os.path.join(self.folder, filename))\n",
    "        \n",
    "        if len(img.shape) == 2:\n",
    "            img = gray2rgb(img)\n",
    "        \n",
    "        img = resize(img.astype(np.float32), self.image_size, mode='reflect')/255.0\n",
    "        \n",
    "        return img\n",
    "    \n",
    "    def __loadpredict__(self, filename):\n",
    "        # load jpg file as numpy array\n",
    "        img = imread(os.path.join(self.folder, filename))\n",
    "        \n",
    "        if len(img.shape) == 2:\n",
    "            img = gray2rgb(img)\n",
    "            \n",
    "        # resize both image and mask\n",
    "        img = resize(img.astype(np.float32), self.image_size, mode='reflect')/255.0 \n",
    "        return img\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        # select batch\n",
    "        filenames = self.filenames[index*self.batch_size:(index+1)*self.batch_size]\n",
    "        # predict mode: return images and filenames\n",
    "        if self.predict:\n",
    "            # load files\n",
    "            imgs = [self.__loadpredict__(filename) for filename in filenames]\n",
    "            # create numpy batch\n",
    "            imgs = np.array(imgs)\n",
    "            return imgs, filenames\n",
    "        # train mode: return images and masks\n",
    "        else:\n",
    "            # load files\n",
    "            imgs = [self.__load__(filename) for filename in filenames]\n",
    "            \n",
    "            # create numpy batch\n",
    "            imgs = np.array(imgs)\n",
    "            \n",
    "            return imgs, imgs\n",
    "        \n",
    "    def on_epoch_end(self):\n",
    "        if self.shuffle:\n",
    "            random.shuffle(self.filenames)\n",
    "        \n",
    "    def __len__(self):\n",
    "        if self.predict:\n",
    "            # return everything\n",
    "            return int(np.ceil(len(self.filenames) / self.batch_size))\n",
    "        else:\n",
    "            # return full batches only\n",
    "            return int(len(self.filenames) / self.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "835a54b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train and valid data generators\n",
    "train_gen = generator(TRAIN_FOLDER, train_filenames)\n",
    "valid_gen = generator(TRAIN_FOLDER, valid_filenames)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76649fa5",
   "metadata": {},
   "source": [
    "### Conv stacked Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d497964b",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_encoder = keras.models.Sequential([\n",
    "    keras.layers.Input(shape=(256, 256, 3)),\n",
    "    keras.layers.Conv2D(16, kernel_size=3, padding=\"SAME\", activation=\"selu\"),\n",
    "    keras.layers.MaxPool2D(pool_size=2),\n",
    "    keras.layers.Conv2D(32, kernel_size=3, padding=\"SAME\", activation=\"selu\"),\n",
    "    keras.layers.MaxPool2D(pool_size=2),\n",
    "    keras.layers.Conv2D(64, kernel_size=3, padding=\"SAME\", activation=\"selu\"),\n",
    "    keras.layers.MaxPool2D(pool_size=2),\n",
    "    keras.layers.Conv2D(128, kernel_size=3, padding=\"SAME\", activation=\"selu\"),\n",
    "    keras.layers.MaxPool2D(pool_size=2),\n",
    "    keras.layers.Conv2D(256, kernel_size=3, padding=\"SAME\", activation=\"selu\"),\n",
    "    keras.layers.MaxPool2D(pool_size=2),\n",
    "    keras.layers.Conv2D(512, kernel_size=3, padding=\"SAME\", activation=\"selu\"),\n",
    "    keras.layers.MaxPool2D(pool_size=2),\n",
    "    keras.layers.Conv2D(1024, kernel_size=3, padding=\"SAME\", activation=\"selu\"),\n",
    "    keras.layers.MaxPool2D(pool_size=2),\n",
    "])\n",
    "conv_decoder = keras.models.Sequential([\n",
    "    keras.layers.Conv2DTranspose(512, kernel_size=3, strides=2, padding=\"SAME\", activation=\"selu\", \n",
    "                                 input_shape=[2, 2, 1024]),\n",
    "    keras.layers.Conv2DTranspose(256, kernel_size=3, strides=2, padding=\"SAME\", activation=\"selu\"),\n",
    "    keras.layers.Conv2DTranspose(128, kernel_size=3, strides=2, padding=\"SAME\", activation=\"selu\"),\n",
    "    keras.layers.Conv2DTranspose(64, kernel_size=3, strides=2, padding=\"SAME\", activation=\"selu\"),\n",
    "    keras.layers.Conv2DTranspose(32, kernel_size=3, strides=2, padding=\"SAME\", activation=\"selu\"),\n",
    "    keras.layers.Conv2DTranspose(16, kernel_size=3, strides=2, padding=\"SAME\", activation=\"selu\"),\n",
    "    keras.layers.Conv2DTranspose(3, kernel_size=3, strides=2, padding=\"SAME\", activation=\"sigmoid\")\n",
    "])\n",
    "conv_ae = keras.models.Sequential([conv_encoder, conv_decoder])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72ecab8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model compile\n",
    "conv_ae.compile(optimizer=Adam(1e-4), \n",
    "                loss='mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cdfa497",
   "metadata": {},
   "outputs": [],
   "source": [
    "#add ModelCheckpoint and EarlyStopping callbacks\n",
    "weight_path=\"{}_weights_best.h5\".format(MODEL_WEIGHTS_PATH+'auto_encoder')\n",
    "\n",
    "checkpoint = ModelCheckpoint(weight_path, monitor='val_loss', verbose=1, \n",
    "                             save_best_only=True, mode='min', save_weights_only = True)\n",
    "\n",
    "early = EarlyStopping(monitor=\"val_loss\", \n",
    "                      mode=\"min\", \n",
    "                      patience=5)\n",
    "\n",
    "callbacks_list = [checkpoint, early]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f466b028",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training model\n",
    "loss_history = conv_ae.fit(train_gen,\n",
    "                           epochs=N_EPOCHS,\n",
    "                           validation_data=valid_gen,\n",
    "                           callbacks=callbacks_list\n",
    "                              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85aa8977",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,4))\n",
    "plt.plot(loss_history.epoch, loss_history.history[\"loss\"], label=\"Train loss\")\n",
    "plt.plot(loss_history.epoch, loss_history.history[\"val_loss\"], label=\"Valid loss\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc5f922e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7c1b597d",
   "metadata": {},
   "source": [
    "## Sparse Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f850dfbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train and valid data generators\n",
    "train_gen_dence = generator(TRAIN_FOLDER, train_filenames, image_size=(70, 50))\n",
    "valid_gen_dence = generator(TRAIN_FOLDER, valid_filenames, image_size=(70, 50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef740d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_l1_encoder = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[70, 50, 3]),\n",
    "    keras.layers.Dense(5000, activation=\"selu\"),\n",
    "    keras.layers.Dense(2000, activation=\"selu\"),\n",
    "    keras.layers.Dense(1000, activation=\"selu\"),\n",
    "    keras.layers.Dense(500, activation=\"selu\"),\n",
    "    keras.layers.Dense(250, activation=\"sigmoid\"),\n",
    "    keras.layers.ActivityRegularization(l1=1e-3)  # Alternatively, you could add\n",
    "                                                  # activity_regularizer=keras.regularizers.l1(1e-3)\n",
    "                                                  # to the previous layer.\n",
    "])\n",
    "sparse_l1_decoder = keras.models.Sequential([\n",
    "    keras.layers.Dense(500, activation=\"selu\", input_shape=[250]),\n",
    "    keras.layers.Dense(1000, activation=\"selu\"),\n",
    "    keras.layers.Dense(2000, activation=\"selu\"),\n",
    "    keras.layers.Dense(5000, activation=\"selu\"),\n",
    "    keras.layers.Dense(70 * 50 * 3, activation=\"sigmoid\"),\n",
    "    keras.layers.Reshape([70, 50, 3])\n",
    "])\n",
    "sparse_l1_ae = keras.models.Sequential([sparse_l1_encoder, sparse_l1_decoder])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe1e4c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model compile\n",
    "sparse_l1_ae.compile(optimizer=Adam(1e-4), \n",
    "                loss='mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f94222e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#add ModelCheckpoint and EarlyStopping callbacks\n",
    "weight_path=\"{}_weights_best.h5\".format(MODEL_WEIGHTS_PATH+'auto_encoder_sparse')\n",
    "\n",
    "checkpoint = ModelCheckpoint(weight_path, monitor='val_loss', verbose=1, \n",
    "                             save_best_only=True, mode='min', save_weights_only = True)\n",
    "\n",
    "early = EarlyStopping(monitor=\"val_loss\", \n",
    "                      mode=\"min\", \n",
    "                      patience=5)\n",
    "\n",
    "callbacks_list = [checkpoint, early]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ab5d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training model\n",
    "loss_history = sparse_l1_ae.fit(train_gen_dence,\n",
    "                           epochs=N_EPOCHS,\n",
    "                           validation_data=valid_gen_dence,\n",
    "                           callbacks=callbacks_list\n",
    "                              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "213785d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,4))\n",
    "plt.plot(loss_history.epoch, loss_history.history[\"loss\"], label=\"Train loss\")\n",
    "plt.plot(loss_history.epoch, loss_history.history[\"val_loss\"], label=\"Valid loss\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e03594e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b9b71059",
   "metadata": {},
   "source": [
    "## PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab18c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train and valid data generators\n",
    "train_gen_pca = generator(TRAIN_FOLDER, train_filenames, batch_size=3000, image_size=(200, 150))\n",
    "valid_gen_pca = generator(TRAIN_FOLDER, valid_filenames, image_size=(200, 150))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb231ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=3000)\n",
    "\n",
    "for items in train_gen_pca:\n",
    "    imgs = items[0]\n",
    "    break\n",
    "    \n",
    "    \n",
    "flatten_imgs = np.array([x.flatten() for x in imgs])\n",
    "pca.fit(flatten_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ffbf16",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6a443d0a",
   "metadata": {},
   "source": [
    "# Part 5 - present_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03d704b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# image names to combine\n",
    "test_filenames = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90de1f1e",
   "metadata": {},
   "source": [
    "## Conv Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce3402f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_gen = generator(TRAIN_FOLDER, test_filenames, predict=True, batch_size=len(test_filenames))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aced5813",
   "metadata": {},
   "outputs": [],
   "source": [
    "for items in test_gen:\n",
    "    \n",
    "    imgs_for_print = items[0]\n",
    "    \n",
    "pred_for_mix = conv_encoder.predict(imgs_for_print)\n",
    "    \n",
    "mean = np.mean(pred_for_mix, axis=0)\n",
    "#mean = np.median(pred_for_mix, axis=0)\n",
    "\n",
    "mean = np.expand_dims(mean, axis=0)\n",
    "pred = conv_decoder.predict(mean)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3001e6a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(resize(pred[0], (200, 150)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "387eff2e",
   "metadata": {},
   "source": [
    "## Sparse Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e20160",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_gen_sparse = generator(TRAIN_FOLDER, test_filenames, predict=True, batch_size=len(test_filenames), \n",
    "                            image_size=(70,50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3240bec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for items in test_gen_sparse:\n",
    "    \n",
    "    imgs_for_print = items[0]\n",
    "    \n",
    "pred_for_mix = sparse_l1_encoder.predict(imgs_for_print)\n",
    "    \n",
    "mean = np.mean(pred_for_mix, axis=0)\n",
    "#mean = np.median(pred_for_mix, axis=0)\n",
    "\n",
    "mean = np.expand_dims(mean, axis=0)\n",
    "pred = sparse_l1_decoder.predict(mean)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afcca55e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(resize(pred[0], (200, 150)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "449273ec",
   "metadata": {},
   "source": [
    "## PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c20958",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_gen_pca = generator(TRAIN_FOLDER, test_filenames, predict=True, batch_size=len(test_filenames), \n",
    "                            image_size=(200,150))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5277b0bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "for items in test_gen:\n",
    "    imgs = items[0]\n",
    "    break\n",
    "\n",
    "flatten_imgs = np.array([x.flatten() for x in imgs])\n",
    "flatten_imgs_post = pca.transform(flatten_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aaef7d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = np.mean(flatten_imgs_post, axis=0)\n",
    "\n",
    "mean = pca.inverse_transform(mean)\n",
    "\n",
    "mean = mean.reshape(200, 150, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fef6bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(mean)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09825feb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d3690af",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
